{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f638b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "image_dir = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Amazon ML Challenge-20251010T213227Z-1-001\\Amazon ML Challenge\\student_resource\\images\"\n",
    "prices_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\Amazon ML Challenge-20251010T213227Z-1-001\\Amazon ML Challenge\\student_resource\\dataset\\train.csv\"\n",
    "\n",
    "# 1) Enumerate image files and derive sample_ids\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.jpg')]\n",
    "image_sample_ids = [os.path.splitext(f)[0] for f in image_files]\n",
    "\n",
    "# 2) Load prices to align with images\n",
    "prices_df = pd.read_csv(prices_path)\n",
    "prices_df = prices_df.drop(['catalog_content', 'image_link'], axis=1)\n",
    "prices_df['sample_id'] = prices_df['sample_id'].astype(str)\n",
    "\n",
    "# 3) Sanity checks\n",
    "print(f\"Found {len(image_sample_ids)} images.\")\n",
    "print(f\"Prices dataset has {len(prices_df)} rows.\")\n",
    "print(\"Block 1 ready: image sample_ids derived and alignment plan established.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63388fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"DATASET ALIGNMENT\")\n",
    "\n",
    "# 1. Set paths\n",
    "image_dir = \"./images\"  # Original images folder\n",
    "prices_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\amazon_ml_challenge\\Amazon ML Challenge\\student_resource\\dataset\\train.csv\"\n",
    "\n",
    "# 2. Get sample_ids from images\n",
    "jpg_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.jpg')]\n",
    "image_sample_ids = [os.path.splitext(f)[0] for f in jpg_files]\n",
    "image_set = set(image_sample_ids)\n",
    "\n",
    "# 3. Load prices\n",
    "prices_df = pd.read_csv(prices_path)\n",
    "prices_df['sample_id'] = prices_df['sample_id'].astype(str)\n",
    "price_set = set(prices_df['sample_id'].tolist())\n",
    "\n",
    "# 4. Find alignment\n",
    "common_ids = image_set & price_set\n",
    "missing_in_images = price_set - image_set\n",
    "missing_in_prices = image_set - price_set\n",
    "\n",
    "# 5. Save aligned list\n",
    "aligned_sample_ids = sorted(list(common_ids))\n",
    "np.save(\"aligned_sample_ids.npy\", np.array(aligned_sample_ids, dtype=str))\n",
    "\n",
    "# Optional: save missing\n",
    "if missing_in_images:\n",
    "    pd.Series(list(missing_in_images)).to_csv(\"missing_images.csv\", index=False, header=[\"sample_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622486d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "print(\"IMAGE RESIZING (ONE-TIME)\")\n",
    "\n",
    "\n",
    "# Paths\n",
    "input_dir = \"./images\"\n",
    "output_dir = \"./images_resized\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load aligned IDs (only resize images we'll use)\n",
    "aligned_ids = np.load(\"aligned_sample_ids.npy\").astype(str)\n",
    "\n",
    "# Resize with error handling\n",
    "corrupted = []\n",
    "successful = 0\n",
    "\n",
    "for sid in tqdm(aligned_ids, desc=\"Resizing images\"):\n",
    "    input_path = os.path.join(input_dir, f\"{sid}.jpg\")\n",
    "    output_path = os.path.join(output_dir, f\"{sid}.jpg\")\n",
    "    try:\n",
    "        img = Image.open(input_path)\n",
    "        img.load()\n",
    "        img_resized = img.resize((256, 256), Image.BICUBIC)\n",
    "        img_resized.save(output_path, quality=95, optimize=True)\n",
    "        successful += 1\n",
    "    except Exception as e:\n",
    "        corrupted.append(sid)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nsuccessfully resized {successful} images.\")\n",
    "print(f\"Skipped {len(corrupted)} corrupted images.\")\n",
    "\n",
    "# Update aligned list (remove corrupted)\n",
    "if corrupted:\n",
    "    aligned_clean = [sid for sid in aligned_ids if sid not in corrupted]\n",
    "    np.save(\"aligned_sample_ids_cleaned.npy\", np.array(aligned_clean, dtype=str))\n",
    "else:\n",
    "    np.save(\"aligned_sample_ids_cleaned.npy\", aligned_ids)\n",
    "\n",
    "# Persist corrupted list if any\n",
    "if corrupted:\n",
    "    with open(\"corrupted_images.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(corrupted))\n",
    "\n",
    "print(\"BLOCK 1.6 COMPLETE - Use './images_resized' in Block 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f119f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "\n",
    "\n",
    "print(\"BLOCK 2 - IMAGE LOADER\")\n",
    "\n",
    "\n",
    "# Paths\n",
    "image_dir = \"./images_resized\"\n",
    "aligned_sample_ids = np.load(\"aligned_sample_ids_cleaned.npy\").astype(str)\n",
    "\n",
    "# Transform\n",
    "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "transform = weights.transforms()\n",
    "\n",
    "# Dataset\n",
    "class ImageDatasetBySampleID(Dataset):\n",
    "    def __init__(self, sample_ids, image_dir, transform=None):\n",
    "        self.sample_ids = sample_ids\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = str(self.sample_ids[idx])\n",
    "        path = os.path.join(self.image_dir, f\"{sid}.jpg\")\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return sid, image\n",
    "\n",
    "# DataLoader\n",
    "dataset = ImageDatasetBySampleID(aligned_sample_ids, image_dir, transform=transform)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created.\")\n",
    "print(f\"Total batches: {len(loader)}\")\n",
    "print(f\"Batch size: {loader.batch_size}\")\n",
    "\n",
    "# Test first batch\n",
    "print(\"\\nTesting First Batch\")\n",
    "start = time.time()\n",
    "sids, imgs = next(iter(loader))\n",
    "elapsed = time.time() - start\n",
    "print(f\"First batch loaded in {elapsed:.2f} seconds.\")\n",
    "print(f\"Shape: {imgs.shape}\")\n",
    "\n",
    "\n",
    "print(\"BLOCK 2 COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f889e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"BLOCK 3 - EMBEDDING EXTRACTION (Vision Transformer)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Model (ViT-B/16)\n",
    "\n",
    "\n",
    "print(\"\\nLoading Model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "model.heads.head = nn.Identity()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Feature dimension: 768 (ViT-B/16 output)\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTesting First Batch Inference\")\n",
    "\n",
    "start_test = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nExtracting Embeddings\")\n",
    "emb_list = []\n",
    "sid_list = []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(loader, total=len(loader), desc=\"ViT: Extracting\")\n",
    "    for sids, imgs in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        feats = model(imgs)\n",
    "        feats_np = feats.cpu().numpy().astype(np.float16)\n",
    "        emb_list.append(feats_np)\n",
    "        sid_list.extend(list(sids))\n",
    "        if len(emb_list) % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = len(sid_list) / elapsed\n",
    "            pbar.set_postfix({\"Speed\": f\"{speed:.1f} img/s\"})\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSaving Embeddings\")\n",
    "embeddings = np.vstack(emb_list)\n",
    "np.save(\"image_embeddings_vit.npy\", embeddings)\n",
    "np.save(\"image_sample_ids_order.npy\", np.array(sid_list, dtype=str))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "avg_speed = len(sid_list) / total_time\n",
    "\n",
    "print(f\"Embeddings saved!\")\n",
    "print(f\"Shape: {embeddings.shape}\")  # (N, 768)\n",
    "print(f\"Total time: {total_time/60:.2f} minutes\")\n",
    "\n",
    "\n",
    "print(\"\\nSummary\")\n",
    "print(f\"Model: Vision Transformer B/16\")\n",
    "print(f\"Feature dimension: {embeddings.shape[1]}\")\n",
    "print(f\"Total images processed: {len(sid_list)}\")\n",
    "print(f\"Saved to: image_embeddings_vit.npy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e455a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "\n",
    "print(\"BLOCK 4 - PCA ON IMAGE EMBEDDINGS\")\n",
    "\n",
    "\n",
    "# Load embeddings\n",
    "print(\"\\nLoading Embeddings\")\n",
    "embeddings = np.load(\"image_embeddings_vit.npy\")  # [N, D]\n",
    "N, D = embeddings.shape\n",
    "print(f\"Loaded embeddings: {embeddings.shape}\")\n",
    "\n",
    "# PCA configuration\n",
    "D_pca = 512\n",
    "print(f\"\\nPCA Configuration\")\n",
    "print(f\"Target dimension: {D_pca}\")\n",
    "print(f\"Full dataset: {N} samples, {D} â†’ {D_pca} features\")\n",
    "\n",
    "# Fit PCA on full dataset (unsupervised preprocessing)\n",
    "print(\"\\nFitting PCA\")\n",
    "pca = PCA(n_components=D_pca, random_state=42)\n",
    "pca.fit(embeddings)\n",
    "\n",
    "# Transform\n",
    "embeddings_pca = pca.transform(embeddings)\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving Results\")\n",
    "output_emb_path = f\"image_embeddings_pca{D_pca}_full.npy\"\n",
    "np.save(output_emb_path, embeddings_pca)\n",
    "print(f\"Saved embeddings: {output_emb_path} ({embeddings_pca.shape})\")\n",
    "\n",
    "pkl_path = f\"image_pca_{D_pca}.pkl\"\n",
    "joblib.dump(pca, pkl_path)\n",
    "print(f\"Saved PCA model: {pkl_path}\")\n",
    "\n",
    "# Diagnostics\n",
    "print(\"\\nPCA Diagnostics\")\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance: {explained_variance:.4f} ({explained_variance*100:.2f}%)\")\n",
    "\n",
    "np.save(f\"pca_explained_variance_ratio_{D_pca}.npy\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25327f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"ALIGN IMAGE + TEXT EMBEDDINGS WITH PRICES\")\n",
    "\n",
    "\n",
    "# 1. Load Image Embeddings\n",
    "image_sample_ids = np.load(\"image_sample_ids_order.npy\").astype(str)\n",
    "embeddings_img = np.load(\"image_embeddings_pca512_full.npy\")  # [N, 512]\n",
    "assert embeddings_img.shape[0] == len(image_sample_ids), \"Image IDs and embeddings count mismatch.\"\n",
    "\n",
    "df_images = pd.DataFrame({\n",
    "    \"sample_id\": image_sample_ids,\n",
    "    \"img_emb\": list(embeddings_img)\n",
    "})\n",
    "df_images[\"sample_id\"] = df_images[\"sample_id\"].astype(str)\n",
    "\n",
    "# 2. Load Text Embeddings\n",
    "text_df = pd.read_csv(r\"C:\\Users\\hp\\OneDrive\\Desktop\\amazon_ml_challenge\\Amazon ML Challenge\\student_resource\\text_embeddings_with_id_1.csv\")\n",
    "text_df[\"sample_id\"] = text_df[\"sample_id\"].astype(str)\n",
    "all_cols = list(text_df.columns)\n",
    "text_cols = [c for c in all_cols if c != \"sample_id\"]\n",
    "\n",
    "# 3. Load Prices\n",
    "prices_df = pd.read_csv(\n",
    "    r\"C:\\Users\\hp\\OneDrive\\Desktop\\amazon_ml_challenge\\Amazon ML Challenge\\student_resource\\dataset\\train.csv\"\n",
    ")\n",
    "prices_df[\"sample_id\"] = prices_df[\"sample_id\"].astype(str)\n",
    "\n",
    "price_col = \"price\" if \"price\" in prices_df.columns else [c for c in prices_df.columns if \"price\" in c.lower()][0]\n",
    "\n",
    "# 4. Merge by sample_id\n",
    "df_join = prices_df.merge(df_images, on=\"sample_id\", how=\"inner\")\n",
    "df_join = df_join.merge(text_df[[\"sample_id\"] + text_cols], on=\"sample_id\", how=\"inner\")\n",
    "\n",
    "if len(df_join) == 0:\n",
    "    raise ValueError(\"No matching sample_ids after merge. Check sample_id formats.\")\n",
    "\n",
    "# 5. Build features\n",
    "X_img = np.vstack(df_join[\"img_emb\"].values).astype(np.float32)\n",
    "X_txt = df_join[text_cols].to_numpy(dtype=np.float32)\n",
    "X = np.hstack([X_img, X_txt]).astype(np.float32)\n",
    "y = df_join[price_col].to_numpy()\n",
    "\n",
    "# Applying log tranform on prices\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "print(f\"âœ… Log range: {y_log.min():.3f} - {y_log.max():.3f}\")\n",
    "\n",
    "\n",
    "# 6. Save\n",
    "np.save(\"X_combined_image_text.npy\", X)\n",
    "np.save(\"y_price.npy\", y_log)\n",
    "np.save(\"aligned_sample_ids_final.npy\", df_join[\"sample_id\"].values)\n",
    "\n",
    "# 7. Validation\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y_log.shape}\")\n",
    "\n",
    "\n",
    "print(\"FINAL FEATURE SUMMARY\")\n",
    "print(f\"Image features: {X_img.shape[1]} dims from ViT embeddings\")\n",
    "print(f\"Text features: {X_txt.shape[1]} dims from 205 text features\")\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Training samples: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# SMAPE metric\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    smape_values = np.where(denominator != 0, numerator / denominator, 0)\n",
    "    return np.mean(smape_values) * 100\n",
    "\n",
    "def lgb_smape_metric(y_pred, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    y_true_actual = np.expm1(y_true)\n",
    "    y_pred_actual = np.expm1(y_pred)\n",
    "    smape_value = calculate_smape(y_true_actual, y_pred_actual)\n",
    "    return 'smape', smape_value, False\n",
    "\n",
    "# Load data\n",
    "X = np.load(\"X_combined_image_text.npy\")\n",
    "y = np.load(\"y_price.npy\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "val_set = lgb.Dataset(X_val, label=y_val, reference=train_set, free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l1',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 30,\n",
    "    'max_depth': 8,\n",
    "    'feature_fraction': 0.58,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 70,\n",
    "    'lambda_l2': 5.0,\n",
    "    'lambda_l1': 0.3,\n",
    "    'min_gain_to_split': 0.4,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=1200,\n",
    "    valid_sets=[train_set, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=30),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ],\n",
    "    feval=lgb_smape_metric\n",
    ")\n",
    "\n",
    "y_pred_log = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val_actual, y_pred)\n",
    "smape_score = calculate_smape(y_val_actual, y_pred)\n",
    "\n",
    "print(f\"MAE: ${mae:.2f}\")\n",
    "print(f\"SMAPE: {smape_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f212a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# SMAPE Metric Implementation\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    Formula: (1/n) * Î£ |predicted - actual| / ((|actual| + |predicted|)/2) * 100\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual values\n",
    "        y_pred: predicted values\n",
    "    \n",
    "    Returns:\n",
    "        SMAPE score (0-200%, lower is better)\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    smape_values = np.where(denominator != 0, numerator / denominator, 0)\n",
    "    smape_score = np.mean(smape_values) * 100\n",
    "    \n",
    "    return smape_score\n",
    "\n",
    "\n",
    "def lgb_smape_metric(y_pred, train_data):\n",
    "    \"\"\"\n",
    "    Custom SMAPE metric for LightGBM\n",
    "    Returns: (metric_name, metric_value, is_higher_better)\n",
    "    \"\"\"\n",
    "    y_true = train_data.get_label()\n",
    "    \n",
    "    # If using log-transformed prices, reverse transform\n",
    "    y_true_actual = np.expm1(y_true)\n",
    "    y_pred_actual = np.expm1(y_pred)\n",
    "    \n",
    "    smape_value = calculate_smape(y_true_actual, y_pred_actual)\n",
    "    \n",
    "    return 'smape', smape_value, False  # False = lower is better\n",
    "\n",
    "\n",
    "# Load Data\n",
    "\n",
    "X = np.load(\"X_combined_image_text.npy\")  # [N, D]\n",
    "y = np.load(\"y_price.npy\")                   # [N]\n",
    "\n",
    "print(f\"Loaded data: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "\n",
    "\n",
    "# Train-Val Split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Val set: {X_val.shape[0]} samples\")\n",
    "\n",
    "\n",
    "# Create LightGBM Datasets\n",
    "\n",
    "train_set = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "val_set = lgb.Dataset(X_val, label=y_val, reference=train_set, free_raw_data=False)\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l1',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 24,\n",
    "    'max_depth': 8,\n",
    "    'feature_fraction': 0.58,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 100,\n",
    "    'lambda_l2': 5.0,\n",
    "    'lambda_l1': 0.3,\n",
    "    'min_gain_to_split': 0.4,\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "# Train Model with SMAPE\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=1200,\n",
    "    valid_sets=[train_set, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=30),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ],\n",
    "    feval=lgb_smape_metric  # Add SMAPE as custom metric\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Training completed. Best iteration: {gbm.best_iteration}\")\n",
    "\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_log = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_val_actual, y_pred)\n",
    "print(f\"MAE (Mean Absolute Error): ${mae:.2f}\")\n",
    "\n",
    "# Calculate SMAPE\n",
    "smape_score = calculate_smape(y_val_actual, y_pred)\n",
    "print(f\"SMAPE (Symmetric Mean Absolute Percentage Error): {smape_score:.2f}%\")\n",
    "print(f\"  â†’ Lower is better (range: 0-200%)\")\n",
    "\n",
    "# Example interpretation\n",
    "print(f\"\\nðŸ“Š Interpretation:\")\n",
    "print(f\"   - If actual price = $100 and predicted = $120\")\n",
    "print(f\"   - SMAPE = |100-120| / ((|100|+|120|)/2) * 100 = 18.18%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a828b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model_block7_no_pca.txt\"\n",
    "gbm.save_model(model_path)\n",
    "print(f\"\\n Model saved to: {model_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
